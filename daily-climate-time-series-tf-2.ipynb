{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-25T08:06:29.339515Z","iopub.execute_input":"2022-01-25T08:06:29.339893Z","iopub.status.idle":"2022-01-25T08:06:29.359835Z","shell.execute_reply.started":"2022-01-25T08:06:29.339801Z","shell.execute_reply":"2022-01-25T08:06:29.359155Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# import libraries\nimport numpy as np\nimport pandas as pd \nimport tensorflow as tf\nfrom dateutil.parser import parse\ndateparse=lambda dates:parse(dates)\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:06:29.361387Z","iopub.execute_input":"2022-01-25T08:06:29.361766Z","iopub.status.idle":"2022-01-25T08:06:31.014898Z","shell.execute_reply.started":"2022-01-25T08:06:29.361728Z","shell.execute_reply":"2022-01-25T08:06:31.013970Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# load data from csv file and parse dates \ntrain_data = pd.read_csv('../input/daily-climate-time-series-data/DailyDelhiClimateTrain.csv',\n                         parse_dates=['date'],date_parser=dateparse)\ntrain_data","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:06:31.016037Z","iopub.execute_input":"2022-01-25T08:06:31.016246Z","iopub.status.idle":"2022-01-25T08:06:31.122543Z","shell.execute_reply.started":"2022-01-25T08:06:31.016220Z","shell.execute_reply":"2022-01-25T08:06:31.121494Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv('../input/daily-climate-time-series-data/DailyDelhiClimateTest.csv',\n                         parse_dates=['date'],date_parser=dateparse)\ntest_data","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:06:31.124198Z","iopub.execute_input":"2022-01-25T08:06:31.124922Z","iopub.status.idle":"2022-01-25T08:06:31.153259Z","shell.execute_reply.started":"2022-01-25T08:06:31.124876Z","shell.execute_reply":"2022-01-25T08:06:31.152433Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# check cloumn data types\nprint(train_data.dtypes)\nprint(test_data.dtypes)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:06:31.155450Z","iopub.execute_input":"2022-01-25T08:06:31.155964Z","iopub.status.idle":"2022-01-25T08:06:31.162712Z","shell.execute_reply.started":"2022-01-25T08:06:31.155931Z","shell.execute_reply":"2022-01-25T08:06:31.161753Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# convert data and menatemp column into numpy arraies for both test and training dfs\ntrain_series = np.array(train_data.meantemp)\ntrain_time = np.array(train_data.date)\ntest_series = np.array(test_data.meantemp)\ntest_time = np.array(test_data.date)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:06:31.163871Z","iopub.execute_input":"2022-01-25T08:06:31.164651Z","iopub.status.idle":"2022-01-25T08:06:31.174748Z","shell.execute_reply.started":"2022-01-25T08:06:31.164610Z","shell.execute_reply":"2022-01-25T08:06:31.173858Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# print shapes \nprint(train_series.shape)\nprint(test_series.shape)\nprint(train_time.shape)\nprint(test_time.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:06:31.177694Z","iopub.execute_input":"2022-01-25T08:06:31.178125Z","iopub.status.idle":"2022-01-25T08:06:31.187031Z","shell.execute_reply.started":"2022-01-25T08:06:31.178089Z","shell.execute_reply":"2022-01-25T08:06:31.186237Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# plot meantep and date columns \nplt.figure(figsize=(10, 6))\nplt.plot(train_time,train_series)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:06:31.188312Z","iopub.execute_input":"2022-01-25T08:06:31.188533Z","iopub.status.idle":"2022-01-25T08:06:31.430004Z","shell.execute_reply.started":"2022-01-25T08:06:31.188507Z","shell.execute_reply":"2022-01-25T08:06:31.429047Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"window_size = 33 # 33 time steps that will be used as features to predict the next time step in the time series \nbatch_size = 50 # the number of training examples in one forward/backward pass throught the network.\n                # 1000/50 = 20 groups of 50 will be proagated throught the network \nshuffle_buffer_size = 500 # shuffle data so that its not in any order ","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:06:31.431535Z","iopub.execute_input":"2022-01-25T08:06:31.431838Z","iopub.status.idle":"2022-01-25T08:06:31.436735Z","shell.execute_reply.started":"2022-01-25T08:06:31.431799Z","shell.execute_reply":"2022-01-25T08:06:31.435774Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# convert dataset into training data that the model can use\n\n# function inputs: data series, size of the window,The size of the batches to use when training,\n# the size of the shuffle buffer, which determines how the data will be shuffled.\n# Expand the dimensions of the series\n# Create dataset ds from the series\n# Slice the data up into the appropriate windows, shifted by one time set.\n# keep them all the same size by setting drop remainder to true.\n# flatten the data into numpy array in the size of our window_size + 1.\n# shuffle data so that its not in order\n# than split data into features and labels  \n\ndef windowed_dataset(series, window_size, batch_size, shuffle_buffer_size):\n    series = tf.expand_dims(series, axis=-1)\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n    ds = ds.shuffle(shuffle_buffer_size)\n    ds = ds.map(lambda w: (w[:-1], w[1:]))\n    return ds.batch(batch_size).prefetch(1)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:06:31.438028Z","iopub.execute_input":"2022-01-25T08:06:31.438867Z","iopub.status.idle":"2022-01-25T08:06:31.452419Z","shell.execute_reply.started":"2022-01-25T08:06:31.438824Z","shell.execute_reply":"2022-01-25T08:06:31.451327Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# helper function that can preform forecasting after training model\ndef model_forecast(model, series, window_size):\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size))\n    ds = ds.batch(50).prefetch(1)\n    forecast = model.predict(ds)\n    return forecast","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:06:31.454087Z","iopub.execute_input":"2022-01-25T08:06:31.454657Z","iopub.status.idle":"2022-01-25T08:06:31.463777Z","shell.execute_reply.started":"2022-01-25T08:06:31.454621Z","shell.execute_reply":"2022-01-25T08:06:31.462866Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"window_size","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:06:31.465512Z","iopub.execute_input":"2022-01-25T08:06:31.466020Z","iopub.status.idle":"2022-01-25T08:06:31.477890Z","shell.execute_reply.started":"2022-01-25T08:06:31.465975Z","shell.execute_reply":"2022-01-25T08:06:31.477335Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# used to find the optimum learning rate for the optimizer (form of hyperparameter tuning) using conv1D, LSTM, and dense layer\ntf.keras.backend.clear_session() # this helps avoid clutter from old models and layers\ntf.random.set_seed(51)  # to get reproducable results \nnp.random.seed(51)  # generate pseudo-random numbers \n\ntrain_set = windowed_dataset(train_series, window_size, batch_size, shuffle_buffer_size)\nprint(train_set)\nprint(train_series.shape)\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Conv1D(filters=32, kernel_size=5,\n                      strides=1, padding=\"causal\",\n                      activation=\"relu\",\n                      input_shape=[None, 1]),\n  tf.keras.layers.LSTM(64, return_sequences=True),\n  tf.keras.layers.LSTM(64, return_sequences=True),\n  tf.keras.layers.Dense(30, activation=\"relu\"),\n  tf.keras.layers.Dense(10, activation=\"relu\"),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 100) # scale outputs by 100 using lambda layer \n])\n\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(\n    lambda epoch: 1e-8 * 10**(epoch / 20)) # to tune the learning rate set up a callback \n\noptimizer = tf.keras.optimizers.SGD(learning_rate=1e-8, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])\nhistory = model.fit(train_set, epochs=100, callbacks=[lr_schedule])","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:06:31.478813Z","iopub.execute_input":"2022-01-25T08:06:31.479616Z","iopub.status.idle":"2022-01-25T08:10:51.422322Z","shell.execute_reply.started":"2022-01-25T08:06:31.479581Z","shell.execute_reply":"2022-01-25T08:10:51.421453Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# plot the learning rate loss \nplt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\nplt.axis([1e-8, 1e-3, 0, 30])","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:10:51.425288Z","iopub.execute_input":"2022-01-25T08:10:51.425566Z","iopub.status.idle":"2022-01-25T08:10:52.326653Z","shell.execute_reply.started":"2022-01-25T08:10:51.425534Z","shell.execute_reply":"2022-01-25T08:10:52.325773Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# optimum learning rate found was headed towards 1e-4 \ntf.keras.backend.clear_session() # this helps avoid clutter from old models and layers\ntf.random.set_seed(51)\nnp.random.seed(51)\n#batch_size = 16\ndataset = windowed_dataset(train_series, window_size, batch_size, shuffle_buffer_size)\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Conv1D(filters=32, kernel_size=3,\n                      strides=1, padding=\"causal\",\n                      activation=\"relu\",\n                      input_shape=[None, 1]),\n  tf.keras.layers.LSTM(32, return_sequences=True),\n  tf.keras.layers.LSTM(32, return_sequences=True),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 100)\n])\n\noptimizer = tf.keras.optimizers.SGD(learning_rate=1e-4, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])\nhistory = model.fit(dataset,epochs=100)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:10:52.327779Z","iopub.execute_input":"2022-01-25T08:10:52.328003Z","iopub.status.idle":"2022-01-25T08:14:07.654913Z","shell.execute_reply.started":"2022-01-25T08:10:52.327977Z","shell.execute_reply":"2022-01-25T08:14:07.654050Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"window_size = 1\nforecast = model_forecast(model, test_series[..., np.newaxis], window_size)\nforecast = forecast[window_size:-1, -1, 0]\nprint(forecast.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:14:07.656336Z","iopub.execute_input":"2022-01-25T08:14:07.656653Z","iopub.status.idle":"2022-01-25T08:14:08.617118Z","shell.execute_reply.started":"2022-01-25T08:14:07.656621Z","shell.execute_reply":"2022-01-25T08:14:08.616034Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.plot(test_time, test_series) #  plot test time and its corrsponding meantep as test_series\nplt.plot(test_time[:112], forecast) #  plot test time first 112 and the predicted meantep \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:14:08.618359Z","iopub.execute_input":"2022-01-25T08:14:08.618849Z","iopub.status.idle":"2022-01-25T08:14:08.867630Z","shell.execute_reply.started":"2022-01-25T08:14:08.618816Z","shell.execute_reply":"2022-01-25T08:14:08.866647Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"print(\"First 7 Predictions :\",\"\\n\", forecast[:7])\nprint('')\nprint('Actual first 7 values:', \"\\n\", test_series[:7])","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:14:08.869081Z","iopub.execute_input":"2022-01-25T08:14:08.869344Z","iopub.status.idle":"2022-01-25T08:14:08.875846Z","shell.execute_reply.started":"2022-01-25T08:14:08.869310Z","shell.execute_reply":"2022-01-25T08:14:08.875219Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"print('Mean absolute error:')\ntf.keras.metrics.mean_absolute_error(test_series[:112], forecast).numpy()","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:14:08.877188Z","iopub.execute_input":"2022-01-25T08:14:08.877463Z","iopub.status.idle":"2022-01-25T08:14:08.894066Z","shell.execute_reply.started":"2022-01-25T08:14:08.877431Z","shell.execute_reply":"2022-01-25T08:14:08.893027Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# predictions on the left and actual values on the right \nm_p = pd.Series(forecast,test_series[:112])\nm_p","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:14:08.896588Z","iopub.execute_input":"2022-01-25T08:14:08.896908Z","iopub.status.idle":"2022-01-25T08:14:08.908123Z","shell.execute_reply.started":"2022-01-25T08:14:08.896866Z","shell.execute_reply":"2022-01-25T08:14:08.907465Z"},"trusted":true},"execution_count":20,"outputs":[]}]}